{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the objective of this file is scrape yahoo finance web page: get financial info about companies and add that info to a google sheet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this section creates a list of tickers from a google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from googleapiclient import discovery\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function reads a sheet and gets tickers\n",
    "def get_tickers(spreadsheet_id,range_):\n",
    "    ## here we make sure we get authorize\n",
    "    scope = 'https://www.googleapis.com/auth/spreadsheets.readonly'\n",
    "\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('creds.json', scope)\n",
    "\n",
    "    service = discovery.build('sheets', 'v4', credentials=creds)\n",
    "    \n",
    "    ## here we read the sheet an extract the tickers and remember the row of each ticker\n",
    "    sheet = service.spreadsheets()\n",
    "    data = sheet.values().get(spreadsheetId=spreadsheet_id, range=range_).execute()\n",
    "    values = data['values']\n",
    "    tickers = []\n",
    "    row = 0\n",
    "    for item in values:\n",
    "        if item == [] or row < 5:\n",
    "            row += 1\n",
    "            continue\n",
    "        elif len(tickers) < 6:\n",
    "            ticker = re.findall('\\((.*)\\)', item[0])\n",
    "            ticker = ticker[0].lower().strip()\n",
    "            tickers.append(ticker)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return tickers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this section receives the list of tickers and get the financial info from yahoo for each ticker in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the tickers\n",
    "tickers = get_tickers('1R0h3A6cmfqbCV7788cLK3DdyRirREs8KbHxppulbf98', 'hoja 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making a class Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self, ticker):\n",
    "        self._ticker = ticker\n",
    "        url = self._search_ticker(self._ticker)\n",
    "        if url == None:\n",
    "            print('the page took too long to load for ticker', self._ticker)\n",
    "            self._html = None\n",
    "        else:\n",
    "            ticker_page = requests.get(url)\n",
    "            self._html = BeautifulSoup(ticker_page.text, 'html.parser')\n",
    "\n",
    "\n",
    "    def _search_ticker(self, ticker):\n",
    "        ## tiempo maximo para la demora inteligente\n",
    "        delay = 15\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path='./chromedriver')\n",
    "        driver.get('https://finance.yahoo.com/')\n",
    "\n",
    "        try:\n",
    "            ## making sure the page loaded\n",
    "            page_loaded = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH, '//form[@id=\"header-search-form\"]/input[@id=\"yfin-usr-qry\"]')))\n",
    "        \n",
    "            ## searching ticker\n",
    "            search_box = driver.find_element_by_xpath('//form[@id=\"header-search-form\"]/input       [@id=\"yfin-usr-qry\"]')\n",
    "            search_box.send_keys(self._ticker)\n",
    "            search_button = driver.find_element_by_xpath('//form[@id=\"header-search-form\"]//button[@id=\"header-desktop-search-button\"]')\n",
    "            search_button.click()\n",
    "\n",
    "            ## making sure the page loaded\n",
    "            page_loaded = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH, '//div[@id=\"quote-header-info\"]')))\n",
    "\n",
    "            ## getting url, making the request and the soup\n",
    "            url = driver.current_url\n",
    "\n",
    "            return url\n",
    "       \n",
    "        except TimeoutException:\n",
    "            print('the page took too long to load')\n",
    "\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using only functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function gets the financial info from yahoo finance\n",
    "def get_financials(ticker):\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver')\n",
    "    url = 'https://finance.yahoo.com/'\n",
    "    driver.get(url)\n",
    "\n",
    "    ## tiempo maximo para la demora inteligente\n",
    "    delay = 15\n",
    "\n",
    "    try:\n",
    "        ## waiting for the page\n",
    "        page_loaded = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH, '//form[@id=\"header-search-form\"]/input[@id=\"yfin-usr-qry\"]')))\n",
    "        \n",
    "        ## searching ticker\n",
    "        search_box = driver.find_element_by_xpath('//form[@id=\"header-search-form\"]/input       [@id=\"yfin-usr-qry\"]')\n",
    "        search_box.send_keys(ticker)\n",
    "        search_button = driver.find_element_by_xpath('//form[@id=\"header-search-form\"]//button[@id=\"header-desktop-search-button\"]')\n",
    "        search_button.click()\n",
    "\n",
    "        ## waiting for the page\n",
    "        page_loaded = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH, '//div[@id=\"quote-header-info\"]')))\n",
    "\n",
    "        ## getting url, making the request and the soup\n",
    "        url = driver.current_url\n",
    "        ticker_page = requests.get(url)\n",
    "        s = BeautifulSoup(ticker_page.text, 'html.parser') \n",
    "\n",
    "        ## creating financial_info dict and getting company, ticker, exchange, price, price_currency \n",
    "        financial_info = {}\n",
    "        \n",
    "\n",
    "    except TimeoutException:\n",
    "        print('the page took too long to load\\n or the ticker was not found')\n",
    "\n",
    "    ## close the browser\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this section sends the financial info to a google sheet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}